{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model, load_model, model_from_yaml\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skater.core.local_interpretation.dnni.deep_interpreter import DeepInterpreter\n",
    "from skater.core.visualizer.text_relevance_visualizer import build_visual_explainer\n",
    "from skater.util.dataops import convert_dataframe_to_dict, show_in_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TensorFlow session and register it with Keras. It will use this session to initialize all the variables\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "embedding_dims = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "#### IMDB dataset: \n",
    "##### 1. http://ai.stanford.edu/~amaas//data/sentiment/\n",
    "##### 2. http://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf ( Section 4.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "# The Dataset contains 50,000 reviews(Train:25,000 and Test:25,000)\n",
    "# More info about the dataset: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> shown in australia as <UNK> this incredibly bad movie is so bad that you become <UNK> and have to watch it to the end just to see if it could get any worse and it does the storyline is so predictable it seems written by a high school dramatics class the sets are pathetic but marginally better than the <UNK> and the acting is wooden br br the infant <UNK> seems to have been stolen from the props cupboard of <UNK> <UNK> there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\n",
      "\n",
      "Length: 129\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "# Reading raw text\n",
    "INDEX_FROM = 3\n",
    "# Get the {Word: Index} mapping\n",
    "word_to_id = imdb.get_word_index()\n",
    "\n",
    "def adjust_word_id_offset(word_id_dict):\n",
    "    word_id_dict = {k:(v+INDEX_FROM) for k,v in word_id_dict.items()}\n",
    "    word_id_dict[\"<PAD>\"] = 0\n",
    "    word_id_dict[\"<START>\"] = 1\n",
    "    word_id_dict[\"<UNK>\"] = 2\n",
    "    return word_id_dict\n",
    "\n",
    "w_to_id = adjust_word_id_offset(word_to_id)\n",
    "\n",
    "def get_raw_txt(word_id_dict, input_data):\n",
    "    id_to_word = {value:key for key,value in word_id_dict.items()}\n",
    "    return ' '.join([(id_to_word[_id] if _id in id_to_word else 'None') for _id in input_data])\n",
    "\n",
    "r_t = get_raw_txt(w_to_id, x_train[20])\n",
    "print(r_t + \"\\n\")\n",
    "print(\"Length: {}\".format(len(r_t.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dramatics class the sets are pathetic but marginally better than the <UNK> and the acting is wooden br br the infant <UNK> seems to have been stolen from the props cupboard of <UNK> <UNK> there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\n",
      "\n",
      "Length: 80\n"
     ]
    }
   ],
   "source": [
    "# Raw text post selecting the top most frequently occurring words\n",
    "index_train = 20\n",
    "r_t_r = get_raw_txt(w_to_id, x_train[index_train])\n",
    "print(r_t_r + \"\\n\")\n",
    "print(\"Length: {}\".format(len(r_t_r.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim=max_features,\n",
    "                    output_dim=embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 37s - loss: 0.4450 - acc: 0.7754 - val_loss: 0.3360 - val_acc: 0.8521\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 37s - loss: 0.2316 - acc: 0.9087 - val_loss: 0.3780 - val_acc: 0.8414\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 37s - loss: 0.0917 - acc: 0.9683 - val_loss: 0.4550 - val_acc: 0.8410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feab6881438>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24768/25000 [============================>.] - ETA: 0s\n",
      "\n",
      "\n",
      "Train score: 0.02479563113629818\n",
      "Train accuracy: 0.99412\n",
      "\n",
      "\n",
      "Test score: 0.45503118330955505\n",
      "Test accuracy: 0.84104\n"
     ]
    }
   ],
   "source": [
    "# Compute train and test accuracy using cross entropy as the cost function\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score_test, acc_test = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "score_train, acc_train = model.evaluate(x_train, y_train,\n",
    "                            batch_size=batch_size)\n",
    "print(\"\\n\\n\")\n",
    "print('Train score:', score_train)\n",
    "print('Train accuracy:', acc_train)\n",
    "print(\"\\n\")\n",
    "print('Test score:', score_test)\n",
    "print('Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 78, 250)           96250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,719,251\n",
      "Trainable params: 2,719,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save and persist the trained keras model in YAML format\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model_cnn_imdb_{}.yaml\".format(epochs), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_cnn_imdb_{}.h5\".format(epochs))\n",
    "print(\"Save model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from disk\n",
      "24736/25000 [============================>.] - ETA: 0s\n",
      "\n",
      "\n",
      "Test score: 0.45503118330955505\n",
      "Test accuracy: 0.84104\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "# Learning phase is set to '0' as we are not training\n",
    "K.set_learning_phase(0)\n",
    "yaml_file = open('model_cnn_imdb_{}.yaml'.format(epochs), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('model_cnn_imdb_{}.h5'.format(epochs))\n",
    "print(\"Load model from disk\")\n",
    "\n",
    "\n",
    "# Validate model performance with the reload of persisted model\n",
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "score, acc = loaded_model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print(\"\\n\\n\")\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets ask Skater to help us in interpreting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_phase 0\n",
      "Load model from disk\n",
      "\r",
      "1/1 [==============================] - 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:52:55,041 - IntegratedGradients - INFO - Executing operations to compute relevance using Integrated Gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class Probabilities: [[0.8115846]]\n",
      "Ground Truth: [1]\n"
     ]
    }
   ],
   "source": [
    "index = 45\n",
    "K.set_learning_phase(0)\n",
    "with DeepInterpreter(session=K.get_session()) as di:\n",
    "    print(\"learning_phase {}\".format(K.learning_phase()))\n",
    "    yaml_file = open('model_cnn_imdb_{}.yaml'.format(epochs), 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    \n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights('model_cnn_imdb_{}.h5'.format(epochs))\n",
    "    print(\"Load model from disk\")    \n",
    "    \n",
    "    # Input data\n",
    "    xs = np.array([x_test[index]])\n",
    "    ys = np.array([y_test[index]])\n",
    "\n",
    "    print('Predicted class Probabilities: {}'.format(loaded_model.predict_proba(np.array([x_test[index]]))))\n",
    "    print('Ground Truth: {}'.format(ys))\n",
    "    \n",
    "    embedding_tensor = loaded_model.layers[0].output\n",
    "    input_tensor = loaded_model.layers[0].input\n",
    "    \n",
    "    embedding_out = di.session.run(embedding_tensor, {input_tensor: xs});\n",
    "    # Using Integrated Gradient for computing feature relevance\n",
    "    relevance_scores = di.explain('ig', loaded_model.layers[-2].output * ys, \n",
    "                                  loaded_model.layers[1].input, embedding_out, use_case='txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    80.000000\n",
       "mean      0.000135\n",
       "std       0.005111\n",
       "min      -0.022063\n",
       "25%      -0.001603\n",
       "50%      -0.000032\n",
       "75%       0.001329\n",
       "max       0.018699\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a dataframe with columns 'features' and 'relevance scores'\n",
    "# Since, the relevance score is compute over the embedding vector, we aggregate it by computing 'mean'\n",
    "# over the embedding to get scalar coefficient for the features\n",
    "relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)\n",
    "relevance_scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent tale of two boys that do whatever they can to get away from there abusive <UNK> father lord of the rings star elijah wood is outstanding in this unforgettable role this movie is one of the main reasons i haven't touched a single beer and never will as long as i live that might make me sound like a nerd but that's what i have to say it is a wonder why this isn't as a classic american tale\n"
     ]
    }
   ],
   "source": [
    "# # Retrieve the text\n",
    "r_t_test = get_raw_txt(w_to_id, x_test[index])\n",
    "print(r_t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:58:45,301 - skater.core.visualizer.text_relevance_visualizer - INFO - Rank order feature relevance based on input created and saved as feature_relevance.png\n",
      "2018-06-12 05:58:45,302 - skater.core.visualizer.text_relevance_visualizer - INFO - Relevance plot name: feature_relevance.png\n",
      "2018-06-12 05:58:45,329 - skater.core.visualizer.text_relevance_visualizer - INFO - Visual Explainer built, use show_in_notebook to render in Jupyter style Notebooks: rendered.html\n"
     ]
    }
   ],
   "source": [
    "build_visual_explainer(r_t_test, relevance_scores_df, highlight_oov=True, enable_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:58:50,852 - skater.util.dataops - INFO - File Name: ./rendered.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<body><h3>Word Relevance</h3><div class=\"row\" style=background-color:#F5F5F5 white-space: pre-wrap; font-size: 12pt; font-family: Avenir Black><span style=\"background-color: rgba(255, 245, 240, 0.7)\">excellent</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(225, 237, 248, 0.7)\">two</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">boys</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">do</span> <span style=\"background-color: rgba(255, 245, 240, 0.7)\">whatever</span> <span style=\"background-color: rgba(254, 216, 199, 0.7)\">they</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">can</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(245, 249, 254, 0.7)\">get</span> <span style=\"background-color: rgba(255, 240, 233, 0.7)\">away</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">from</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">there</span> <span style=\"background-color: rgba(242, 248, 253, 0.7)\">abusive</span> <span style=\"background-color: rgba(242, 248, 253, 0.5)\"><UNK></span> <span style=\"background-color: rgba(253, 202, 181, 0.7)\">father</span> <span style=\"background-color: rgba(237, 244, 252, 0.7)\">lord</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">rings</span> <span style=\"background-color: rgba(231, 240, 250, 0.7)\">star</span> <span style=\"background-color: rgba(244, 249, 254, 0.7)\">elijah</span> <span style=\"background-color: rgba(211, 227, 243, 0.7)\">wood</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(229, 239, 249, 0.7)\">outstanding</span> <span style=\"background-color: rgba(246, 250, 255, 0.7)\">in</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(232, 241, 250, 0.7)\">unforgettable</span> <span style=\"background-color: rgba(228, 239, 249, 0.7)\">role</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(254, 232, 222, 0.7)\">movie</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">one</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(208, 225, 242, 0.7)\">main</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">reasons</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(252, 194, 170, 0.5)\">haven't</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">touched</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(233, 242, 250, 0.7)\">single</span> <span style=\"background-color: rgba(220, 234, 246, 0.7)\">beer</span> <span style=\"background-color: rgba(227, 238, 249, 0.7)\">and</span> <span style=\"background-color: rgba(220, 233, 246, 0.7)\">never</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">will</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">long</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(250, 104, 73, 0.7)\">live</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(180, 211, 233, 0.7)\">might</span> <span style=\"background-color: rgba(106, 174, 214, 0.7)\">make</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">me</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">sound</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">like</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">nerd</span> <span style=\"background-color: rgba(245, 250, 254, 0.7)\">but</span> <span style=\"background-color: rgba(245, 250, 254, 0.5)\">that's</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">what</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(255, 239, 232, 0.7)\">have</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(255, 244, 239, 0.7)\">say</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">it</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(8, 48, 107, 0.7)\">wonder</span> <span style=\"background-color: rgba(93, 165, 209, 0.7)\">why</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(204, 223, 241, 0.5)\">isn't</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(227, 238, 248, 0.7)\">classic</span> <span style=\"background-color: rgba(241, 247, 253, 0.7)\">american</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span></div><div align=\"center\"><img src=\"./feature_relevance.png?6\"</div></body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_in_notebook('./rendered.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating conditional adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text by deleting or updating textual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent tale of two boys that do whatever they can to get away from there abusive <UNK> father lord of the rings star elijah wood is <UNK> in this unforgettable role this movie is one of the main reasons i haven't touched a single beer and never will as long as i live that might make me sound like a nerd but that's what i have to say it is a wonder why this isn't as a classic american tale\n"
     ]
    }
   ],
   "source": [
    "new_txt = r_t_test.replace(\"outstanding\", \"<UNK>\")\n",
    "print(new_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to engineered feature format\n",
    "# Reference: https://stackoverflow.com/questions/42964375/how-to-input-new-text-for-prediction-in-keras-while-using-an-inbuilt-dataset\n",
    "\n",
    "def input_formatter_imdb(input_txt, word_index_mapping):\n",
    "    \n",
    "    x_i_test = [[word_index_mapping[wrds] if wrds in word_index_mapping.keys() else word_index_mapping[\"<UNK>\"] \n",
    "                 for wrds in input_txt.split(' ')]]\n",
    "    x_i_test = sequence.pad_sequences(x_i_test, maxlen=maxlen)\n",
    "    txt_vector = np.array([x_i_test.flatten()])\n",
    "    return txt_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  321,   787,     7,   107,  1013,    15,    81,   845,    36,\n",
       "           70,     8,    79,   245,    39,    50,  4595,     2,   336,\n",
       "         1635,     7,     4,  2666,   323, 13245,  2137,     9,     2,\n",
       "           11,    14,  3210,   217,    14,    20,     9,    31,     7,\n",
       "            4,   293,  1007,    13,   774,  2842,     6,   686,  3640,\n",
       "            5,   115,    80,    17,   196,    17,    13,   412,    15,\n",
       "          238,    97,    72,   481,    40,     6,  5155,    21,   198,\n",
       "           51,    13,    28,     8,   135,    12,     9,     6,   594,\n",
       "          138,    14,   218,    17,     6,   356,   298,   787]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = input_formatter_imdb(new_txt, w_to_id)\n",
    "input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_phase 0\n",
      "Load model from disk\n",
      "\r",
      "1/1 [==============================] - 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:56:40,355 - IntegratedGradients - INFO - Executing operations to compute relevance using Integrated Gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class : [[0.8115846]]\n",
      "Ground Truth: [1]\n",
      "(1, 80, 128)\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(0)\n",
    "with DeepInterpreter(session=K.get_session()) as di:\n",
    "    print(\"learning_phase {}\".format(K.learning_phase()))\n",
    "    yaml_file = open('model_cnn_imdb_{}.yaml'.format(epochs), 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    \n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights('model_cnn_imdb_{}.h5'.format(epochs))\n",
    "    print(\"Load model from disk\")    \n",
    "    \n",
    "    # Input data\n",
    "    xs = input_vector\n",
    "    ys = np.array([1])\n",
    "\n",
    "    print('Predicted class : {}'.format(loaded_model.predict_proba(np.array([x_test[index]]))))\n",
    "    print('Ground Truth: {}'.format(ys))\n",
    "    \n",
    "    embedding_tensor = loaded_model.layers[0].output\n",
    "    input_tensor = loaded_model.layers[0].input\n",
    "    \n",
    "    embedding_out = di.session.run(embedding_tensor, {input_tensor: xs})\n",
    "    print(embedding_out.shape)\n",
    "    # Using Integrated Gradient for computing feature relevance\n",
    "    relevance_scores = di.explain('ig', loaded_model.layers[-2].output * ys, \n",
    "                                  loaded_model.layers[1].input, embedding_out, use_case='txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Document:\n",
      "\n",
      " excellent tale of two boys that do whatever they can to get away from there abusive <UNK> father lord of the rings star elijah wood is <UNK> in this unforgettable role this movie is one of the main reasons i haven't touched a single beer and never will as long as i live that might make me sound like a nerd but that's what i have to say it is a wonder why this isn't as a classic american tale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:58:21,121 - skater.core.visualizer.text_relevance_visualizer - INFO - Rank order feature relevance based on input created and saved as feature_relevance.png\n",
      "2018-06-12 05:58:21,122 - skater.core.visualizer.text_relevance_visualizer - INFO - Relevance plot name: feature_relevance.png\n",
      "2018-06-12 05:58:21,140 - skater.core.visualizer.text_relevance_visualizer - INFO - Visual Explainer built, use show_in_notebook to render in Jupyter style Notebooks: rendered.html\n",
      "2018-06-12 05:58:21,141 - skater.util.dataops - INFO - File Name: ./rendered.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<body><h3>Word Relevance</h3><div class=\"row\" style=background-color:#F5F5F5 white-space: pre-wrap; font-size: 12pt; font-family: Avenir Black><span style=\"background-color: rgba(255, 245, 240, 0.7)\">excellent</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(225, 237, 248, 0.7)\">two</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">boys</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">do</span> <span style=\"background-color: rgba(255, 245, 240, 0.7)\">whatever</span> <span style=\"background-color: rgba(254, 216, 199, 0.7)\">they</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">can</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(245, 249, 254, 0.7)\">get</span> <span style=\"background-color: rgba(255, 240, 233, 0.7)\">away</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">from</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">there</span> <span style=\"background-color: rgba(242, 248, 253, 0.7)\">abusive</span> <span style=\"background-color: rgba(242, 248, 253, 0.5)\"><UNK></span> <span style=\"background-color: rgba(253, 202, 181, 0.7)\">father</span> <span style=\"background-color: rgba(237, 244, 252, 0.7)\">lord</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">rings</span> <span style=\"background-color: rgba(231, 240, 250, 0.7)\">star</span> <span style=\"background-color: rgba(244, 249, 254, 0.7)\">elijah</span> <span style=\"background-color: rgba(211, 227, 243, 0.7)\">wood</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(254, 219, 204, 0.5)\"><UNK></span> <span style=\"background-color: rgba(246, 250, 255, 0.7)\">in</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(232, 241, 250, 0.7)\">unforgettable</span> <span style=\"background-color: rgba(228, 239, 249, 0.7)\">role</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(254, 232, 222, 0.7)\">movie</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">one</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(208, 225, 242, 0.7)\">main</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">reasons</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(252, 194, 170, 0.5)\">haven't</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">touched</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(233, 242, 250, 0.7)\">single</span> <span style=\"background-color: rgba(220, 234, 246, 0.7)\">beer</span> <span style=\"background-color: rgba(227, 238, 249, 0.7)\">and</span> <span style=\"background-color: rgba(220, 233, 246, 0.7)\">never</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">will</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">long</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(250, 104, 73, 0.7)\">live</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(180, 211, 233, 0.7)\">might</span> <span style=\"background-color: rgba(106, 174, 214, 0.7)\">make</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">me</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">sound</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">like</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">nerd</span> <span style=\"background-color: rgba(245, 250, 254, 0.7)\">but</span> <span style=\"background-color: rgba(245, 250, 254, 0.5)\">that's</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">what</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(255, 239, 232, 0.7)\">have</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(255, 244, 239, 0.7)\">say</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">it</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(8, 48, 107, 0.7)\">wonder</span> <span style=\"background-color: rgba(93, 165, 209, 0.7)\">why</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(204, 223, 241, 0.5)\">isn't</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(227, 238, 248, 0.7)\">classic</span> <span style=\"background-color: rgba(241, 247, 253, 0.7)\">american</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span></div><div align=\"center\"><img src=\"./feature_relevance.png?5\"</div></body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)\n",
    "relevance_scores_df.describe()\n",
    "\n",
    "# Retrieve the text\n",
    "_in = input_vector.reshape(-1)\n",
    "r_t = get_raw_txt(w_to_id, _in)\n",
    "print(\"New Document:\\n\\n {}\".format(r_t))\n",
    "\n",
    "build_visual_explainer(r_t, relevance_scores_df, highlight_oov=True, enable_plot=True)\n",
    "show_in_notebook('./rendered.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> tale of two boys that do whatever they can to get away from there abusive <UNK> father lord of the rings star elijah wood is <UNK> in this <UNK> role this movie is one of the main reasons i haven't <UNK> a single beer and <UNK> will as long as i live that might make me sound like a nerd but that's what i have to say it is a wonder why this isn't as a <UNK> american tale\n"
     ]
    }
   ],
   "source": [
    "# replacing words in a sentence\n",
    "import re\n",
    "words_to_replace = [\"excellent\", 'classic', 'unforgettable', 'touched', 'never']\n",
    "regex_builder = re.compile('|'.join(map(re.escape, words_to_replace)))\n",
    "new_txt2 = regex_builder.sub(\"<UNK>\" , new_txt)\n",
    "print(new_txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vector2 = input_formatter_imdb(new_txt2, w_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_phase 0\n",
      "Load model from disk\n",
      "\r",
      "1/1 [==============================] - 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:56:48,219 - IntegratedGradients - INFO - Executing operations to compute relevance using Integrated Gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class : [[0.8115846]]\n",
      "Ground Truth: [1]\n",
      "(1, 80, 128)\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(0)\n",
    "with DeepInterpreter(session=K.get_session()) as di:\n",
    "    print(\"learning_phase {}\".format(K.learning_phase()))\n",
    "    yaml_file = open('model_cnn_imdb_{}.yaml'.format(epochs), 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    \n",
    "    loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights('model_cnn_imdb_{}.h5'.format(epochs))\n",
    "    print(\"Load model from disk\")    \n",
    "    \n",
    "    # Input data\n",
    "    xs = input_vector2\n",
    "    ys = np.array([1])\n",
    "\n",
    "    print('Predicted class : {}'.format(loaded_model.predict_proba(np.array([x_test[index]]))))\n",
    "    print('Ground Truth: {}'.format(ys))\n",
    "    \n",
    "    embedding_tensor = loaded_model.layers[0].output\n",
    "    input_tensor = loaded_model.layers[0].input\n",
    "    \n",
    "    embedding_out = di.session.run(embedding_tensor, {input_tensor: xs})\n",
    "    print(embedding_out.shape)\n",
    "    # Using Integrated Gradient for computing feature relevance\n",
    "    relevance_scores = di.explain('ig', loaded_model.layers[-2].output * ys, \n",
    "                                  loaded_model.layers[1].input, embedding_out, use_case='txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    80.000000\n",
      "mean     -0.000468\n",
      "std       0.003736\n",
      "min      -0.020762\n",
      "25%      -0.001576\n",
      "50%      -0.000083\n",
      "75%       0.001273\n",
      "max       0.010532\n",
      "dtype: float64\n",
      "New Document:\n",
      "\n",
      " excellent tale of two boys that do whatever they can to get away from there abusive <UNK> father lord of the rings star elijah wood is <UNK> in this unforgettable role this movie is one of the main reasons i haven't touched a single beer and never will as long as i live that might make me sound like a nerd but that's what i have to say it is a wonder why this isn't as a classic american tale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-12 05:56:49,216 - skater.core.visualizer.text_relevance_visualizer - INFO - Rank order feature relevance based on input created and saved as feature_relevance.png\n",
      "2018-06-12 05:56:49,217 - skater.core.visualizer.text_relevance_visualizer - INFO - Relevance plot name: feature_relevance.png\n",
      "2018-06-12 05:56:49,237 - skater.core.visualizer.text_relevance_visualizer - INFO - Visual Explainer built, use show_in_notebook to render in Jupyter style Notebooks: rendered.html\n",
      "2018-06-12 05:56:49,238 - skater.util.dataops - INFO - File Name: ./rendered.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<body><h3>Word Relevance</h3><div class=\"row\" style=background-color:#F5F5F5 white-space: pre-wrap; font-size: 12pt; font-family: Avenir Black><span style=\"background-color: rgba(255, 245, 240, 0.7)\">excellent</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(225, 237, 248, 0.7)\">two</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">boys</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">do</span> <span style=\"background-color: rgba(255, 245, 240, 0.7)\">whatever</span> <span style=\"background-color: rgba(254, 216, 199, 0.7)\">they</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">can</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(245, 249, 254, 0.7)\">get</span> <span style=\"background-color: rgba(255, 240, 233, 0.7)\">away</span> <span style=\"background-color: rgba(254, 227, 214, 0.7)\">from</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">there</span> <span style=\"background-color: rgba(242, 248, 253, 0.7)\">abusive</span> <span style=\"background-color: rgba(242, 248, 253, 0.5)\"><UNK></span> <span style=\"background-color: rgba(253, 202, 181, 0.7)\">father</span> <span style=\"background-color: rgba(237, 244, 252, 0.7)\">lord</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">rings</span> <span style=\"background-color: rgba(231, 240, 250, 0.7)\">star</span> <span style=\"background-color: rgba(244, 249, 254, 0.7)\">elijah</span> <span style=\"background-color: rgba(211, 227, 243, 0.7)\">wood</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(254, 219, 204, 0.5)\"><UNK></span> <span style=\"background-color: rgba(246, 250, 255, 0.7)\">in</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(232, 241, 250, 0.7)\">unforgettable</span> <span style=\"background-color: rgba(228, 239, 249, 0.7)\">role</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(254, 232, 222, 0.7)\">movie</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">one</span> <span style=\"background-color: rgba(254, 222, 207, 0.7)\">of</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">the</span> <span style=\"background-color: rgba(208, 225, 242, 0.7)\">main</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">reasons</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(252, 194, 170, 0.5)\">haven't</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">touched</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(233, 242, 250, 0.7)\">single</span> <span style=\"background-color: rgba(220, 234, 246, 0.7)\">beer</span> <span style=\"background-color: rgba(227, 238, 249, 0.7)\">and</span> <span style=\"background-color: rgba(220, 233, 246, 0.7)\">never</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">will</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">long</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(250, 104, 73, 0.7)\">live</span> <span style=\"background-color: rgba(238, 245, 252, 0.7)\">that</span> <span style=\"background-color: rgba(180, 211, 233, 0.7)\">might</span> <span style=\"background-color: rgba(106, 174, 214, 0.7)\">make</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">me</span> <span style=\"background-color: rgba(255, 244, 238, 0.7)\">sound</span> <span style=\"background-color: rgba(242, 247, 253, 0.7)\">like</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(239, 246, 252, 0.7)\">nerd</span> <span style=\"background-color: rgba(245, 250, 254, 0.7)\">but</span> <span style=\"background-color: rgba(245, 250, 254, 0.5)\">that's</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">what</span> <span style=\"background-color: rgba(252, 194, 170, 0.7)\">i</span> <span style=\"background-color: rgba(255, 239, 232, 0.7)\">have</span> <span style=\"background-color: rgba(247, 251, 255, 0.7)\">to</span> <span style=\"background-color: rgba(255, 244, 239, 0.7)\">say</span> <span style=\"background-color: rgba(254, 233, 223, 0.7)\">it</span> <span style=\"background-color: rgba(254, 219, 204, 0.7)\">is</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(8, 48, 107, 0.7)\">wonder</span> <span style=\"background-color: rgba(93, 165, 209, 0.7)\">why</span> <span style=\"background-color: rgba(204, 223, 241, 0.7)\">this</span> <span style=\"background-color: rgba(204, 223, 241, 0.5)\">isn't</span> <span style=\"background-color: rgba(255, 236, 227, 0.7)\">as</span> <span style=\"background-color: rgba(219, 233, 246, 0.7)\">a</span> <span style=\"background-color: rgba(227, 238, 248, 0.7)\">classic</span> <span style=\"background-color: rgba(241, 247, 253, 0.7)\">american</span> <span style=\"background-color: rgba(255, 242, 236, 0.7)\">tale</span></div><div align=\"center\"><img src=\"./feature_relevance.png?4\"</div></body>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)\n",
    "print(relevance_scores_df.describe())\n",
    "\n",
    "# Retrieve the text\n",
    "_in = input_vector.reshape(-1)\n",
    "r_t = get_raw_txt(w_to_id, _in)\n",
    "print(\"New Document:\\n\\n {}\".format(r_t))\n",
    "\n",
    "build_visual_explainer(r_t, relevance_scores_df, highlight_oov=True, enable_plot=True)\n",
    "show_in_notebook('./rendered.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
