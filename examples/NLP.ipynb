{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NLP Models with Skater\n",
    "\n",
    "In this example, we'll train a couple types of models, and use Skater, LIME, and ipywidgets to interactively explore model behavior.\n",
    "\n",
    "### Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utils\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading np_utils-0.5.3.4.tar.gz (56kB)\n",
      "\u001b[K    100% |################################| 61kB 3.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.0 in /usr/local/lib/python2.7/dist-packages (from np_utils)\n",
      "Collecting future>=0.16 (from np_utils)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |################################| 829kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: np-utils, future\n",
      "  Running setup.py bdist_wheel for np-utils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5d/be/b5/e223535ec3efb733df6afc2518d90d398bbe759e665683b025\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "Successfully built np-utils future\n",
      "Installing collected packages: future, np-utils\n",
      "  Found existing installation: future 0.15.2\n",
      "    Uninstalling future-0.15.2:\n",
      "      Successfully uninstalled future-0.15.2\n",
      "Successfully installed future-0.16.0 np-utils-0.5.3.4\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "Requirement already up-to-date: theano in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from theano)\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano)\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano)\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.3.0-cp27-cp27mu-manylinux1_x86_64.whl (43.1MB)\n",
      "\u001b[K    100% |################################| 43.1MB 31kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow)\n",
      "  Downloading tensorflow_tensorboard-0.1.8-py2-none-any.whl (1.6MB)\n",
      "\u001b[K    100% |################################| 1.6MB 827kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting protobuf>=3.3.0 (from tensorflow)\n",
      "  Downloading protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K    100% |################################| 6.2MB 223kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Requirement already up-to-date: numpy>=1.11.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow)\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting wheel (from tensorflow)\n",
      "  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K    100% |################################| 51kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading Markdown-2.6.9.tar.gz (271kB)\n",
      "\u001b[K    100% |################################| 276kB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |################################| 890kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.3.0->tensorflow)\n",
      "Requirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |################################| 102kB 11.4MB/s a 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: markdown, html5lib\n",
      "  Running setup.py bdist_wheel for markdown ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bf/46/10/c93e17ae86ae3b3a919c7b39dad3b5ccf09aeb066419e5c1e5\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built markdown html5lib\n",
      "Installing collected packages: protobuf, markdown, html5lib, bleach, wheel, tensorflow-tensorboard, backports.weakref, tensorflow, pbr\n",
      "  Found existing installation: protobuf 3.3.0\n",
      "    Uninstalling protobuf-3.3.0:\n",
      "      Successfully uninstalled protobuf-3.3.0\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "  Found existing installation: wheel 0.29.0\n",
      "    Uninstalling wheel-0.29.0:\n",
      "      Successfully uninstalled wheel-0.29.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found existing installation: tensorflow 1.1.0\n",
      "    Uninstalling tensorflow-1.1.0:\n",
      "      Successfully uninstalled tensorflow-1.1.0\n",
      "  Found existing installation: pbr 3.0.1\n",
      "    Uninstalling pbr-3.0.1:\n",
      "      Successfully uninstalled pbr-3.0.1\n",
      "Successfully installed backports.weakref-1.0.post1 bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 pbr-3.1.1 protobuf-3.4.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 wheel-0.30.0\n",
      "Collecting keras==2.0.6\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning.\n",
      "  SNIMissingWarning\n",
      "/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning.\n",
      "  InsecurePlatformWarning\n",
      "  Downloading Keras-2.0.6.tar.gz (228kB)\n",
      "\u001b[K    100% |################################| 235kB 3.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano->keras==2.0.6)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano->keras==2.0.6)\n",
      "Building wheels for collected packages: keras\n",
      "  Running setup.py bdist_wheel for keras ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/80/ba/2beab8c2131e2dcc391ee8a2f55e648af66348115c245e0839\n",
      "Successfully built keras\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.0.8\n",
      "    Uninstalling Keras-2.0.8:\n",
      "      Successfully uninstalled Keras-2.0.8\n",
      "Successfully installed keras-2.0.6\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: preshed<0.47,>=0.46.1 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: sputnik<0.10.0,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: plac in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: thinc<5.1.0,>=5.0.0 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: semver in /usr/local/lib/python2.7/dist-packages (from sputnik<0.10.0,>=0.9.2->spacy)\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install --upgrade np_utils\n",
    "!sudo pip install --upgrade theano\n",
    "!sudo pip install --upgrade tensorflow\n",
    "!sudo pip install keras==2.0.6\n",
    "!sudo pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named spacy.__main__; 'spacy' is a package and cannot be directly executed\r\n"
     ]
    }
   ],
   "source": [
    "### Restart kernel\n",
    "from __future__ import absolute_import\n",
    "\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpaCy Language Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#gimme data\n",
    "dataset = fetch_20newsgroups()\n",
    "docs = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Pretrained Word Embeddings\n",
    "\n",
    "We will use SpaCy's pretrained word embeddings as document representations, and feed these representations into a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.45      0.46      0.46       140\n",
      "           comp.graphics       0.44      0.35      0.39       181\n",
      " comp.os.ms-windows.misc       0.39      0.49      0.44       163\n",
      "comp.sys.ibm.pc.hardware       0.39      0.32      0.35       184\n",
      "   comp.sys.mac.hardware       0.38      0.43      0.40       172\n",
      "          comp.windows.x       0.50      0.53      0.51       186\n",
      "            misc.forsale       0.64      0.67      0.65       182\n",
      "               rec.autos       0.64      0.53      0.58       201\n",
      "         rec.motorcycles       0.49      0.59      0.54       178\n",
      "      rec.sport.baseball       0.63      0.64      0.63       171\n",
      "        rec.sport.hockey       0.70      0.67      0.68       170\n",
      "               sci.crypt       0.63      0.66      0.64       169\n",
      "         sci.electronics       0.49      0.52      0.51       181\n",
      "                 sci.med       0.76      0.72      0.74       177\n",
      "               sci.space       0.59      0.69      0.64       163\n",
      "  soc.religion.christian       0.66      0.67      0.67       201\n",
      "      talk.politics.guns       0.57      0.62      0.59       157\n",
      "   talk.politics.mideast       0.79      0.67      0.73       175\n",
      "      talk.politics.misc       0.43      0.43      0.43       136\n",
      "      talk.religion.misc       0.35      0.20      0.26       108\n",
      "\n",
      "             avg / total       0.55      0.55      0.55      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gimme vectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from spacy.tokens.doc import Doc\n",
    "from sklearn.metrics import classification_report\n",
    "import six\n",
    "\n",
    "def doc2vec(x):\n",
    "    if isinstance(x, (six.binary_type, six.string_types)):\n",
    "        return nlp(x, parse = False, entity = False, tag = False).vector\n",
    "    \n",
    "    elif type(x) in [list, tuple, np.ndarray]:\n",
    "        return np.array([doc2vec(six.text_type(doc)) for doc in x])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Input\") \n",
    "\n",
    "# build a pipeline of text -> vector (transformer), vector -> predictions (model)\n",
    "model = GradientBoostingClassifier(n_estimators = 50)\n",
    "\n",
    "transformer = FunctionTransformer(func = doc2vec, validate=False)\n",
    "pipeline = make_pipeline(transformer, model)\n",
    "pipeline.fit(docs_train, y_train)       \n",
    "\n",
    "#Classification Report on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: CNN \n",
    "In this model, we convert text to a list of padded lists of word IDs, to be used in an embedding lookup table. The embeddings will be trained as part of a CNN implemented with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, corpus, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \"\"\"\n",
    "        corpus: list of strings\n",
    "            Documents used to initialize vocabulary.\n",
    "            \n",
    "        nlp: Spacy language model\n",
    "            If none then will build one in __init__\n",
    "            \n",
    "        max_len: int\n",
    "            Maximum length of a document sequence. Balance information with scale of data.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 1\n",
    "        self.MISSING_VAL = 2\n",
    "        self.START_VAL = 3\n",
    "        self.END_VAL = 4\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()#Counter(['PADDING_VAL','MISSING_VAL','START_VAL','END_VAL'])\n",
    "        self.build_vocab(corpus)\n",
    "\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj) - 2, 0)\n",
    "        we_can_take = self.max_len - 2\n",
    "        result = [self.START_VAL] + obj[:we_can_take] + [self.END_VAL] + [self.PADDING_VAL] * n_pads\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def update(self, words):\n",
    "        for word in words:\n",
    "            self.vocab_counts.update([word])\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()\n",
    "        \n",
    "        for doc in nlp.tokenizer.pipe(map(six.text_type, corpus)):\n",
    "            self.update(map(self._process_token, doc))\n",
    "            \n",
    "        for i, (word, count) in enumerate(self.vocab_counts.most_common(self.max_vocab_size)):\n",
    "            self.vocab[word] = i\n",
    "        \n",
    "    def _process_token(self, token):\n",
    "        if token.is_space:\n",
    "            return \"SPACE\"\n",
    "        elif token.is_punct:\n",
    "            return \"PUNCT\"       \n",
    "        elif token.like_url:\n",
    "            return \"URL\"\n",
    "        elif token.like_email:\n",
    "            return \"EMAIL\"\n",
    "        elif token.like_num:\n",
    "            return \"NUM\"\n",
    "        else:\n",
    "            return token.lower_\n",
    "\n",
    "    def process_token(self, token):\n",
    "        return self.vocab.get(self._process_token(token), self.MISSING_VAL)\n",
    "\n",
    "    def process(self, texts):\n",
    "        docs = []\n",
    "        for doc in self.nlp.tokenizer.pipe(list(texts)):\n",
    "            docs.append(self.pad(list(map(self.process_token, doc))))\n",
    "        return np.array(docs)\n",
    "            \n",
    "    def __call__(self, texts):\n",
    "        return self.process(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Epoch 1/8\n",
      "7919/7919 [==============================] - 23s - loss: 2.7253 - acc: 0.3285    \n",
      "Epoch 2/8\n",
      "7919/7919 [==============================] - 22s - loss: 1.7003 - acc: 0.6727    \n",
      "Epoch 3/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.9550 - acc: 0.7982    \n",
      "Epoch 4/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.5770 - acc: 0.8713    \n",
      "Epoch 5/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.3556 - acc: 0.9244    \n",
      "Epoch 6/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.2166 - acc: 0.9558    \n",
      "Epoch 7/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.1262 - acc: 0.9778    \n",
      "Epoch 8/8\n",
      "7919/7919 [==============================] - 22s - loss: 0.0690 - acc: 0.9910    \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.95      0.88      0.91       140\n",
      "           comp.graphics       0.77      0.76      0.76       181\n",
      " comp.os.ms-windows.misc       0.74      0.83      0.78       163\n",
      "comp.sys.ibm.pc.hardware       0.70      0.75      0.72       184\n",
      "   comp.sys.mac.hardware       0.84      0.81      0.83       172\n",
      "          comp.windows.x       0.81      0.85      0.83       186\n",
      "            misc.forsale       0.83      0.80      0.82       182\n",
      "               rec.autos       0.88      0.82      0.85       201\n",
      "         rec.motorcycles       0.90      0.85      0.87       178\n",
      "      rec.sport.baseball       0.96      0.93      0.94       171\n",
      "        rec.sport.hockey       0.98      0.95      0.96       170\n",
      "               sci.crypt       0.96      0.93      0.95       169\n",
      "         sci.electronics       0.69      0.76      0.72       181\n",
      "                 sci.med       0.87      0.85      0.86       177\n",
      "               sci.space       0.91      0.90      0.90       163\n",
      "  soc.religion.christian       0.89      0.92      0.90       201\n",
      "      talk.politics.guns       0.87      0.92      0.89       157\n",
      "   talk.politics.mideast       0.94      0.97      0.95       175\n",
      "      talk.politics.misc       0.83      0.79      0.81       136\n",
      "      talk.religion.misc       0.79      0.73      0.76       108\n",
      "\n",
      "             avg / total       0.85      0.85      0.85      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convolutional model: https://arxiv.org/abs/1408.5882\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout,  Input, Dense, Activation, Flatten\n",
    "from keras.models import Sequential, Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def model_factory(seq_len, \n",
    "                  vocab_size, \n",
    "                  embedding_size, \n",
    "                  n_classes, \n",
    "                  model_type='sequential',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['acc'], \n",
    "                  optimizer='rmsprop'):\n",
    "    \n",
    "    def create_sequential_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=seq_len))\n",
    "        model.add(Conv1D(64, 3, strides=1, padding='valid'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes,  activation='softmax'))\n",
    "        return model\n",
    "        \n",
    "    def create_non_sequential_model():\n",
    "        _input = Input(shape=(seq_len,), dtype='int32')\n",
    "        _embedding = Embedding(vocab_size, embedding_size, input_length=seq_len)(_input)\n",
    "\n",
    "        # each filter is (3 x 300 ) array of weights\n",
    "        # window (kernel_size) is 3\n",
    "        # so number of weights is (3 * 300 * 64)\n",
    "        # each filter outputs a (200 / strides) x 1 transformation\n",
    "        # padding is how we handle boundaries. include + pad, ignore, etc\n",
    "        _conv_1 = Conv1D(64, 3, strides=1, padding='valid')(_embedding)\n",
    "\n",
    "        # Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        _pool_1 = MaxPooling1D(pool_size=2)(_conv_1)\n",
    "        _conv_2 = Conv1D(64, 3, padding='valid')(_pool_1)\n",
    "        _pool_2 = GlobalMaxPooling1D()(_conv_2) \n",
    "        _activation = Activation('relu')(_pool_2)\n",
    "        output = Dense(n_classes,  activation='softmax')(_activation)\n",
    "        model = Model(inputs=_input, outputs=output)\n",
    "        return model\n",
    "        \n",
    "\n",
    "    def create_model():\n",
    "        if model_type=='sequential':\n",
    "            model = create_sequential_model()\n",
    "        elif model_type == 'non-sequential':\n",
    "            model = create_non_sequential_model()        \n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized model type {}\".format(model_type))\n",
    "\n",
    "        model.compile(loss=loss,\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    return create_model\n",
    "    \n",
    "seq_len = 350\n",
    "vocab_size = 25000\n",
    "embedding_size = 300\n",
    "epochs = 8\n",
    "batch_size = 100\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "model_build = model_factory(seq_len, vocab_size, embedding_size, n_classes)\n",
    "model2 = KerasClassifier(build_fn=model_build, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "processor = FunctionTransformer(TextProcesser(docs_train, nlp=nlp, max_len=seq_len), validate=False)\n",
    "pipeline2 = make_pipeline(processor, model2)\n",
    "\n",
    "# need to one hot encode y labels\n",
    "y2_train = label_binarize(y_train, classes=range(len(np.unique(dataset.target_names))))\n",
    "pipeline2.fit(docs_train, y2_train)\n",
    "\n",
    "# make model silent after training\n",
    "params = model2.get_params()\n",
    "params = {key: value for key, value in params.items() if key != 'build_fn'}\n",
    "params['verbose'] = 0\n",
    "model2.set_params(**params)\n",
    "\n",
    "# Model Performance on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline2.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanations\n",
    "Here, we'll wrap each pipeline into a Skater model object. We'll use this model object to generate LIME explanations in HTML to help better understand how each model makes predictions. We'll wrap this functionality into an ipywidget to allow the user the (a) modify the text and (b) toggle between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: widgetsnbextension>=1.2.6 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: ipykernel>=4.2.2 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: backports.shutil-get-terminal-size; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: pathlib2; python_version == \"2.7\" or python_version == \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: notebook>=4.2.0 in /usr/local/lib/python2.7/dist-packages (from widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: enum34; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.2.1->ipywidgets)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.2.1->ipywidgets)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.2.1->ipywidgets)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python2.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: scandir; python_version < \"3.5\" in /usr/local/lib/python2.7/dist-packages (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython>=4.0.0->ipywidgets)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python2.7/dist-packages (from notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python2.7/dist-packages (from notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python2.7/dist-packages (from notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python2.7/dist-packages (from notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: backports.ssl_match_hostname in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: backports_abc>=0.4 in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.2.2->ipywidgets)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python2.7/dist-packages (from nbformat->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: mistune!=0.6 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python2.7/dist-packages (from jinja2->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: functools32; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: configparser>=3.5; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from entrypoints>=0.2.2->nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Requirement already satisfied: html5lib!=0.9999,!=0.99999,<0.99999999,>=0.999 in /usr/local/lib/python2.7/dist-packages (from bleach->nbconvert->notebook>=4.2.0->widgetsnbextension>=1.2.6->ipywidgets)\n",
      "Collisions detected in /home/deploy/.jupyter/jupyter_notebook_config.py and /home/deploy/.jupyter/jupyter_notebook_config.json config files. /home/deploy/.jupyter/jupyter_notebook_config.json has higher priority: {\n",
      "      \"NotebookApp\": {\n",
      "        \"password\": \"u'sha1:5dc72084d9fd:d561b0390f93dd2181c7b8cbcf5019eeb710939a' ignored, using u'sha1:f7db36e89abc:5026bfe0dc36d0cafb16e8b814f9d1248ec8bbd6'\"\n",
      "      }\n",
      "    }\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Collisions detected in /home/deploy/.jupyter/jupyter_notebook_config.py and /home/deploy/.jupyter/jupyter_notebook_config.json config files. /home/deploy/.jupyter/jupyter_notebook_config.json has higher priority: {\n",
      "      \"NotebookApp\": {\n",
      "        \"password\": \"u'sha1:5dc72084d9fd:d561b0390f93dd2181c7b8cbcf5019eeb710939a' ignored, using u'sha1:f7db36e89abc:5026bfe0dc36d0cafb16e8b814f9d1248ec8bbd6'\"\n",
      "      }\n",
      "    }\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/jupyter-nbextension\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 267, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 900, in start\n",
      "    super(NBExtensionApp, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 256, in start\n",
      "    self.subapp.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 808, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 784, in toggle_nbextension_python\n",
      "    logger=self.log)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 445, in enable_nbextension_python\n",
      "    logger=logger)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 366, in _set_nbextension_state_python\n",
      "    for nbext in nbexts]\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/notebook/nbextensions.py\", line 331, in _set_nbextension_state\n",
      "    cm.update(section, {\"load_extensions\": {require: state}})\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/manager.py\", line 87, in update\n",
      "    self.set(section_name, data)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/manager.py\", line 76, in set\n",
      "    f = open(filename, 'wb')\n",
      "IOError: [Errno 13] Permission denied: u'/usr/etc/jupyter/nbconfig/notebook.json'\n"
     ]
    }
   ],
   "source": [
    "## You may need to enable ipywidgets\n",
    "! pip install ipywidgets\n",
    "!jupyter nbextension enable --py --user widgetsnbextension\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skater\n",
      "  Downloading skater-1.0.2.tar.gz\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Collecting ds-lime>=0.1.1.21 (from skater)\n",
      "  Downloading ds-lime-0.1.1.27.tar.gz (253kB)\n",
      "\u001b[K    100% |################################| 256kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Collecting pathos==0.2.0 (from skater)\n",
      "  Downloading pathos-0.2.0.tgz (68kB)\n",
      "\u001b[K    100% |################################| 71kB 4.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: dill>=0.2.6 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from ds-lime>=0.1.1.21->skater)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Collecting ppft>=1.6.4.5 (from pathos==0.2.0->skater)\n",
      "  Downloading ppft-1.6.4.7.1.zip (78kB)\n",
      "\u001b[K    100% |################################| 81kB 4.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pox>=0.2.2 (from pathos==0.2.0->skater)\n",
      "  Downloading pox-0.2.3.zip (41kB)\n",
      "\u001b[K    100% |################################| 51kB 8.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess>=0.70.4 (from pathos==0.2.0->skater)\n",
      "  Downloading multiprocess-0.70.5.zip (1.5MB)\n",
      "\u001b[K    100% |################################| 1.5MB 872kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil->pandas>=0.19->skater)\n",
      "Building wheels for collected packages: skater, ds-lime, pathos, ppft, pox, multiprocess\n",
      "  Running setup.py bdist_wheel for skater ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2b/80/c4/27748f133a062c246ef28f8916ed8c7bd7f4dc040f3d894be0\n",
      "  Running setup.py bdist_wheel for ds-lime ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/fe/5a/7a7bfca3a84f80ad75d6885ee6e78a887e0b0b480bb2ff0cc6\n",
      "  Running setup.py bdist_wheel for pathos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ac/dd/95/a910070ff4be5079956b2764f61bb8b05e37c7e7563b5e7848\n",
      "  Running setup.py bdist_wheel for ppft ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4d/28/cc/c5bca1e1d3923a5f4dd50219a9e41391bbeb832835e7905344\n",
      "  Running setup.py bdist_wheel for pox ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/b6/98/d03a0a0f153267d1601c3cbaa888ec4cdd52d658a945d44f15\n",
      "  Running setup.py bdist_wheel for multiprocess ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/28/ef/9f/5cc70b5d92fc4641b68dc23b3583f2b6ec1d153cb71985aeaf\n",
      "Successfully built skater ds-lime pathos ppft pox multiprocess\n",
      "Installing collected packages: ds-lime, ppft, pox, multiprocess, pathos, skater\n",
      "Successfully installed ds-lime-0.1.1.27 multiprocess-0.70.5 pathos-0.2.0 pox-0.2.3 ppft-1.6.4.7.1 skater-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install skater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the explorer app.\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from ipywidgets import Button, Textarea, Layout, Box, Label, Text, Output, RadioButtons, HBox\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class TextExplainer(object):\n",
    "    def __init__(self, models, init_pattern=\"\"):\n",
    "        \"\"\"\n",
    "        Display box for LIME results.\n",
    "        \n",
    "        models: dictionary of skater of models.\n",
    "            Keys correspond to user-defined model names, used for radio buttons.\n",
    "            Values are skater models used to generate predictions.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.status = \"Ready\"\n",
    "        self.explainer = LimeTextExplainer(class_names=dataset.target_names)\n",
    "        self.models = models\n",
    "        self.model_names = list(self.models.keys())\n",
    "        self.text_field = Textarea(init_pattern, layout=Layout(height='200px', width='500px'))\n",
    "        self.text_box = Box([Label(value='Text Box'), self.text_field])\n",
    "        \n",
    "        self.status_field = Label(self.status, layout=Layout(height='50px', width='100px'))        \n",
    "        self.status_box = Box([Label(value='Status'), self.status_field])\n",
    "\n",
    "        self.match_button = Button(description='Explain', )\n",
    "        self.match_button.on_click(self.match_pattern)\n",
    "        \n",
    "        self.model_selectors = RadioButtons(\n",
    "            options = self.model_names,\n",
    "            description = \"Use Model\"\n",
    "        )\n",
    "        \n",
    "        self.inputs_box = HBox([self.text_box, self.model_selectors])        \n",
    "        \n",
    "        self.explanation_area = Output()\n",
    "        display(self.inputs_box)       \n",
    "        display(self.match_button)\n",
    "        display(self.status_box)\n",
    "        display(self.explanation_area)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.models[self.model_selectors.value]\n",
    "            \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.text_field.value    \n",
    "    \n",
    "    def match_pattern(self, b):\n",
    "        self.status_field.value = 'loading'\n",
    "        with self.explanation_area:\n",
    "            clear_output()\n",
    "            display(HTML(self.get_explanation_as_html(self.text)))\n",
    "        self.status_field.value = 'ready'\n",
    "\n",
    "    def get_explanation_as_html(self, text):\n",
    "        \n",
    "        # generate most likely class to confine LIME results\n",
    "\n",
    "        explanation = self.explainer.explain_instance(text, \n",
    "                                                      self.model, \n",
    "                                                      top_labels=1)\n",
    "\n",
    "        return explanation.as_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    }
   ],
   "source": [
    "models = {\"CNN\": pipeline2.predict_proba, ' GBC-Pretrain': pipeline.predict_proba}\n",
    "r = TextExplainer(models, docs_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {
    "1b1ff06d921947719db1c103f0437871": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "29f2c8c5857241519968766208a77907": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "325adb080b6c4e438b848829cc17bd3c": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "42733c701e68406d9118db755621bf15": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "625de869ed2748e489e16efc0e540a56": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "7f3679b1950e467a94a70af6caa5b120": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "8d53936726254e99abbb9af1e7a54dfa": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "d71e694e396542dd9cc7f39f31e90d96": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "dea7f67ccc154c2ea3fee2614647a66f": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "e36f3095d8cc4128975d2df3a8dd86c2": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "f156f6209d0341c28bce933e58c06a4c": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    },
    "f54486b71a424245a2b3a178cdb47b7c": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
